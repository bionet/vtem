#!/usr/bin/env python


import numpy as np

import pycuda.driver as cuda
from pycuda.tools import dtype_to_ctype

import utils.parray as parray
from utils.kernel_utils import launch_kernel, func_compile
from utils.simpleio import *
import utils.linalg as la


class dirichlet(object):
    def __init__(self, decode_neuron, tk1, tk2,neuron_ind, kappa, delta, bias, norm, Wt, Mt, dtype = np.float64):
        """
        class dirichlet
        For use to reconstruct videos encoded by VTEMs consist of spatial only receptive fields and IAF neurons
        
        parameters:
        decode_neuron: number of neuron used in the decoding
        tk1: a ndarray vector of float64 containing all the t_k's in \int_{t_k}^{t_{k+1}}
        tk2: a ndarray vector of float64 containing all the t_{k+1}'s in \int_{t_k}^{k+1}}
        neuron_ind: a ndarray vector of int32, where neuron_ind[j] is the neuron index correspond to (tk1[j],tk2[j])
        kappa: a ndarray vector of float64 containing the integration constants for each neuron
        delta: a ndarray vector of float64 containing the threshold for each neuron
        bias:  a ndarray vector of float64 containing the bias for each neuron
        norm:  a ndarray vector of float64 containing the normalization factor for each neuron
               (in case of deterministic threshold, use a vector of ones)
        Wt:  temporal bandwidth
        Mt:  dirichlet order in variable t
        dtype:  np.float64 or np.float32, determine dtype of G matrix and q vector
        
        """
    
    
        self.size = tk1.size
        if self.size == 0:
            raise ValueError("no spikes to decode")
        
        self.Wt = Wt
        self.Mt = Mt
        self.dtype = np.dtype(dtype)
        print "number of spikes: %d" % (self.size)
        
        self.d_tk1 = parray.to_gpu(tk1)
        self.d_tk2 = parray.to_gpu(tk2)
        self.d_neuron_ind = parray.to_gpu(neuron_ind)
        self.d_kappa = parray.to_gpu(kappa)
        self.d_delta = parray.to_gpu(delta)
        self.d_bias = parray.to_gpu(bias)
        self.d_norm = parray.to_gpu(norm)

    def compute_q(self ):
        """ compute q """
        self.d_q = parray.empty((self.size,1), self.dtype)
        
        q_func = get_compute_q_kernel(self.dtype)
        launch_kernel(q_func, (256, 1, 1), (6 * cuda.Context.get_device().MULTIPROCESSOR_COUNT, 1), [self.d_q, self.d_tk1, self.d_tk2, self.d_neuron_ind, self.d_kappa, self.d_delta, self.d_bias, self.d_norm, self.size])

    def compute_G(self, Dswfilename, lamb=0.0):
        """
        compute G matrix using weighting between RFs
        Dswfilename: generated by VTDM_prep
        lamb: smoothing parameter \lambda
        
        """
        
        
        Dsw = read_file(Dswfilename)
        d_Dsw = parray.to_gpu(Dsw)
        del Dsw

        #norm_func = get_put_norm_kernel(d_Dsw.dtype)
        #launch_kernel(norm_func, (256, 1, 1), (d_Dsw.shape[0],1), [d_Dsw, self.d_norm, d_Dsw.ld])

        self.d_G = parray.empty((self.size, self.size), self.dtype)

        G_func = get_G_kernel(self.dtype, d_Dsw.dtype)
        launch_kernel(G_func, (256, 1, 1), (self.d_G.shape[0], 1), [self.d_G, self.d_G.ld, self.d_tk1, self.d_tk2, self.Wt, self.Mt, d_Dsw, d_Dsw.ld, self.d_neuron_ind], timed = "G matrix")

        if lamb != 0:
            lamb_func = get_diag_add_kernel(self.dtype)
            launch_kernel(lamb_func, (256,1,1), (6 * cuda.Context.get_device().MULTIPROCESSOR_COUNT, 1), [self.d_G, self.d_G.ld, self.d_G.shape[0], self.dtype.type(lamb)])

    def compute_Gb(self, Dsfilename, lamb=0.0):
        """
        compute G matrix using dirichlet coefficients
        Dsfilename: generated by VTDM_prepb
        lamb: smoothing parameter \lambda
        
        """
        handle = la.cublashandle()
        import tables
        h5file = tables.openFile(Dsfilename)
        Ds = h5file.root.real.read()
        
        d_Ds = parray.to_gpu(Ds.reshape((Ds.shape[0],-1)))
        del Ds
 
        d_Dsw = parray.empty((d_Ds.shape[0], d_Ds.shape[0]), d_Ds.dtype)
        if d_Ds.dtype == np.float64:
            from scikits.cuda.cublas import cublasDgemm
            gemm = cublasDgemm
        else:
            from scikits.cuda.cublas import cublasSgemm
            gemm = cublasSgemm
        gemm(handle.handle, 't', 'n', d_Dsw.shape[0], d_Dsw.shape[0], d_Ds.shape[1], 1.0, d_Ds.gpudata, d_Ds.ld, d_Ds.gpudata, d_Ds.ld, 0.0, d_Dsw.gpudata, d_Dsw.ld)
        Ds = h5file.root.imag.read()
        d_Ds.set(Ds)
        gemm(handle.handle, 't', 'n', d_Dsw.shape[0], d_Dsw.shape[0], d_Ds.shape[1], 1.0, d_Ds.gpudata, d_Ds.ld, d_Ds.gpudata, d_Ds.ld, 1.0, d_Dsw.gpudata, d_Dsw.ld)        
        del Ds
        h5file.close()
        
        norm_func = get_put_norm_kernel(d_Dsw.dtype)
        launch_kernel(norm_func, (256, 1, 1), (d_Dsw.shape[0],1), [d_Dsw, self.d_norm, d_Dsw.ld])

        self.d_G = parray.empty((self.size, self.size), self.dtype)

        G_func = get_G_kernel(self.dtype, d_Dsw.dtype)
        launch_kernel(G_func, (256, 1, 1), (self.d_G.shape[0], 1), [self.d_G, self.d_G.ld, self.d_tk1, self.d_tk2, self.Wt, self.Mt, d_Dsw, d_Dsw.ld, self.d_neuron_ind], timed = "G matrix")

        if lamb != 0:
            lamb_func = get_diag_add_kernel(self.dtype)
            launch_kernel(lamb_func, (256,1,1), (6 * cuda.Context.get_device().MULTIPROCESSOR_COUNT, 1), [self.d_G, self.d_G.ld, self.d_G.shape[0], self.dtype.type(lamb)])
        
        
    def store_Gq(self):
        """ auxilliary routine, stores G, q to CPU memory
        for use in the situation when insufficient linear memory allocation """
        self.G = self.d_G.get()
        self.q = self.d_q.get()
        
        del self.d_G
        del self.d_q

    def store_others(self):
        """ auxilliary routine, stores tk1, tk2, neuron_ind, norm to CPU memory
        for use in the situation when insufficient linear memory allocation """
        self.tk1 = self.d_tk1.get()
        self.tk2 = self.d_tk2.get()
        self.neuron_ind = self.d_neuron_ind.get()
        self.norm = self.d_norm.get()
        
        del self.d_tk1
        del self.d_tk2
        del self.d_neuron_ind
        del self.d_norm
        
    def restore_Gq(self):
        """ restore GPU memory with G, q """
    
        self.d_G = parray.to_gpu(self.G)
        self.d_q = parray.to_gpu(self.q)

    def restore_others(self):
        """ restore GPU memory with tk1, tk2, neuron_ind, norm """
        self.d_tk1 = parray.to_gpu(self.tk1)
        self.d_tk2 = parray.to_gpu(self.tk2)
        self.d_neuron_ind = parray.to_gpu(self.neuron_ind)
        self.d_norm = parray.to_gpu(self.norm)

        
    def reconstruct(self, dirichfilename, time_frame, dt):
        """
        reconstruct video from c = (G)^{+}q
        
        dirichfilename: generated by either VTDM_prep or VTDM_prepb
        time_frame: a tuple or list of 2, in format [start_time, end_time]
        dt: interval between two consecutive frames in reconstruction
        
        Important Note:
        assumes the solution c is store in self.q
        
        """
    
        t = np.arange(time_frame[0], time_frame[1], dt)
        d_t = parray.to_gpu(t)
        
        dirich = read_file(dirichfilename)
        d_dirich = parray.to_gpu(dirich)
        del dirich
        
        rec_fun = get_reconstruct_kernel(d_dirich.dtype, self.d_q.dtype)
        u_rec = parray.empty((d_t.size, d_dirich.shape[1], d_dirich.shape[2]), np.float64)
        launch_kernel(rec_fun, (128,1,1), ((d_dirich.shape[1]*d_dirich.shape[2]-1) / 128+1, d_t.size), [u_rec, u_rec.ld, d_dirich, d_dirich.ld, self.d_tk1, self.d_tk2, self.d_q, d_t, self.d_neuron_ind, self.d_norm, self.Mt, self.Wt/self.Mt, self.size])
        
        return u_rec
    
    def freeG(self):
        """ free memory used by G, on GPU and CPU, can be called after solving c = (G)^{+}q"""
        del self.d_G
        try:
            del self.G
        except:
            pass
        
       
   

    

def get_compute_q_kernel(dtype):
    template = """
__global__ void
compute_q_Kernel(%(type)s* g_q, double* g_tk1, double* g_tk2, int* neuron_ind, double* kappa, double* delta, double* bias, double* d_norm, int size)
{
	int ind = blockIdx.x * blockDim.x + threadIdx.x;
    int total_threads = blockDim.x * gridDim.x;

	for(int i = ind; i < size; i += total_threads)
	{
		int neuron = neuron_ind[i];

		g_q[i] = (kappa[neuron] * delta[neuron] - bias[neuron] * (g_tk2[i] - g_tk1[i])) * d_norm[neuron];
	}
}

"""
    func = func_compile("compute_q_Kernel", template % {"type": dtype_to_ctype(dtype)})
    return func


def get_put_norm_kernel(dtype):
    template = """
    __global__ void put_norm(%(type)s* d_SWeight, double* d_norm, int ld)
{
	int tid = threadIdx.x;
	int bid = blockIdx.x;
	int NUM_NEURONS = gridDim.x;
	
	double norm1 = d_norm[bid];
	double norm2;

	for(int i = tid; i < NUM_NEURONS; i += blockDim.x)
	{
		norm2 = d_norm[i];
		d_SWeight[i + bid * ld] *= norm1 * norm2;
	}
}
"""
    func = func_compile("put_norm", template  % {"type": dtype_to_ctype(dtype)})
    return func


def get_put_norm_unaligned_kernel(dtype):
    template = """
    __global__ void
put_norm_unaligned(%(type)s* d_SWeight, int ld, double* d_norm, int* neuron_ind1, int* neuron_ind2, int num_neurons1)
{
	int tid = threadIdx.x;
	int bid = blockIdx.x;
	
    int ind1 = neuron_ind1[bid];
	double norm1 = d_norm[ind1];
    int ind2;
	double norm2;

	for(int i = tid; i < num_neurons1; i += blockDim.x)
	{
        ind2 = neuron_ind2[i];
		norm2 = d_norm[ind2];
		d_SWeight[i + bid * ld] *= norm1 * norm2;
	}
}
"""
    func = func_compile("put_norm_unaligned", template  % {"type": dtype_to_ctype(dtype)})
    return func


def get_G_kernel(dtype, dtypew):
    template = """
__device__ double
G_dirichlet_time(double tk1, double tk2, double tl1,double tl2, int M, double WM)
{
	double sum = 0;
	for(int m = 1; m <= M; ++m)
	{
		sum += (cos(m * WM * (tk2-tl2)) - cos(m * WM * (tk2-tl1)) - cos( m * WM * (tk1-tl2)) + cos(m * WM * (tk1-tl1))) / (m*m);
	}

	sum = sum * 2 / (WM * WM) + (tk2-tk1)*(tl2-tl1);
	return sum;
}


    __global__ void
compute_G_Kernel(%(type)s* g_G, int G_ld, double* g_tk1, double* g_tk2, double Wt, int Mt, %(typew)s* SWeight, int Sweight_ld, int* neuron_ind)
{
	unsigned int tid = threadIdx.x;
	unsigned int bdim = blockDim.x;
	unsigned int bid = blockIdx.x;
	int size = gridDim.x;

	double tl[2];
	__shared__ double tk[2];
	__shared__ int ind1;
	int ind2;
	
	
        if(tid == 0)
        {
                tk[0] = g_tk1[bid]; //roundf(g_tk1[bid] * rintf(1 / dt));
        }else if(tid == 1)
        {
                tk[1] = g_tk2[bid]; //roundf(g_tk2[bid] * rintf(1 / dt));
        }else if(tid ==2)
        {
                ind1 = neuron_ind[bid];
        }
                        
        __syncthreads();

        
        for(int i = tid; i < size; i += bdim)
        {
            ind2 = neuron_ind[i];
            tl[0] = g_tk1[i];
            tl[1] = g_tk2[i];
            
            g_G[bid * G_ld + i] = G_dirichlet_time(tk[0],tk[1],tl[0],tl[1],Mt,Wt/Mt) * SWeight[ind1 * Sweight_ld + ind2];

        }	
    
}

"""
    func = func_compile("compute_G_Kernel",  template  % {"type": dtype_to_ctype(dtype), "typew": dtype_to_ctype(dtypew)})
    return func



def get_diag_add_kernel(dtype):
    template = """
__global__ void
diag_add_Kernel(%(type)s* d_G, int ld, int size, %(type)s addin)
{
	int tid = threadIdx.x + blockIdx.x * blockDim.x;
	int total = gridDim.x * blockDim.x;
	
	for(int i = tid; i < size; i+=total)
	{
		d_G[i * ld + i] += addin;
	}

}
"""
    func = func_compile("diag_add_Kernel",  template  % {"type": dtype_to_ctype(dtype)})
    return func

    

def get_reconstruct_kernel(dtype, dtypeq):
    template = """
__global__ void
reconstruct_Kernel(double* u_rec, int u_rec_ld, %(type)s* dirich_space, int dirich_ld, double* g_tk1, double* g_tk2, %(typeq)s* g_ck, double* d_t, int* neuron_ind, double* d_norm, int M, double WM, int size)
{
	unsigned int tid = threadIdx.x;
	unsigned int bid = blockIdx.x;
	unsigned int bdim = blockDim.x;
	unsigned int pix = bid*bdim + tid;

	__shared__ double t;
	
	double u = 0;
	__shared__ double ck[128];
	__shared__ double tk1[128];
	__shared__ double tk2[128];
	__shared__ int ind[128];
	double space;
	double norm;
	
	if(tid == 0)
	{
		t = d_t[blockIdx.y];
	}
	
	
	for(unsigned int i = 0; i < size; i+=bdim)
	{
		if(i + tid < size)
		{
			ck[tid] = g_ck[i + tid];
			tk1[tid] = g_tk1[i + tid];
			tk2[tid] = g_tk2[i + tid];
			ind[tid] = neuron_ind[i + tid];
			
	
		}
		__syncthreads();
		
		for(unsigned int j = 0; j < bdim; ++j)
		{
			if(j + i < size)
			{
				space = dirich_space[ind[j] * dirich_ld + pix];
				norm = d_norm[ind[j]];
				double phi = 0;
				for(int m = 1; m <= M; ++m)
				{
					phi += (sin(m*WM*(t-tk1[j])) - sin(m*WM*(t - tk2[j]))) / m;
				}
				u += ck[j] * (phi * 2 / WM + tk2[j] - tk1[j]) * space * norm;
				
			}
		}
		__syncthreads();
	}
	
	if(pix < u_rec_ld)
	{
            u_rec[pix + u_rec_ld * blockIdx.y] = u;
        }

}

"""
    func = func_compile("reconstruct_Kernel",  template  % {"type": dtype_to_ctype(dtype), "typeq": dtype_to_ctype(dtypeq)})
    return func

