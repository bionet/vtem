#!/usr/bin/env python


import numpy as np
from pycuda.tools import dtype_to_ctype

import utils.parray as parray
from utils.kernel_utils import launch_kernel, func_compile

class pop_encode(object):
    """ Population encoding """
    def __init__(self, num_neurons, dt, dtype = np.float64):
        self.num_neurons = num_neurons
        self.dtype = np.dtype(dtype)
        self.dt = self.dtype.type(dt)





class IAF_encode(pop_encode):
    def __init__(self, num_neurons, dt, dtype = np.float64):
        """
            Population encoding with IAF neurons
            
            Parameters
            ----------
            num_neurons : integer 
                 number of neurons to encode
            dt : float 
                 time interval between two consecutive samples in the input for each neuron
            dtype : np.float64 or np.float32, optional 
                 dtype of spike interval to be stored (np.float32 not yet tested)
                 If not specified, will be set to np.float64
            
            """
        pop_encode.__init__(self,  num_neurons, dt, dtype)
        self.func = get_IAF_kernel_linear(self.dtype)
    
    
    
    def set_time_count(self):
        """ Set time_count zeros """
        self.h_time_count = np.zeros((self.num_neurons,1), self.dtype)
    
    def set_parameters(self):
        """
            Set the parameters using default values
            
            Will set kappa's to be 1.0, deltas to be 0.03, bias to be 0.8
            
            """
        
        self.h_kappa = np.zeros(self.num_neurons, self.dtype) + 1.0
        
        self.h_delta = np.zeros(self.num_neurons, self.dtype) + 0.03
        
        self.h_bias = np.zeros(self.num_neurons, self.dtype) + 0.8
        self.h_sigma = np.zeros(self.num_neurons, self.dtype)
    
    def set_initial_value(self):
        """ Set integration from zeros """
        #self.h_v0 = np.zeros(self.num_neurons, self.dtype)
        self.h_v0 = np.random.uniform(0,1,self.num_neurons).astype(
                    self.dtype) * self.h_delta
    
    def load_parameters(self, h_kappa = None, h_bias = None, h_delta = None, h_sigma = None, h_time_count = None, h_v0 = None):
        """
            Load encoding parameters to GPU
            
            h_kappa, h_bias, h_delta can be set up using default values (using None),
            or specified together (using ndarrays of dtype)
            
            h_time_count can be set up using default values (using None), or specified values(using ndarrays of dtype)
            
            h_v0 can be set up using default values (using None), or specified values(using ndarrays of dtype)
            """
        
        if h_kappa is None:
            self.set_parameters()
        else:
            self.h_kappa = h_kappa
            self.h_delta = h_delta
            self.h_bias = h_bias
            self.h_sigma = np.zeros(h_kappa.shape)
        
        if h_time_count is None:
            self.set_time_count()
        else:
            self.h_time_count = h_time_count
        
        
        if h_v0 is None:
            self.set_initial_value()
        else:
            self.h_v0 = h_v0
        
        self.d_kappa = parray.to_gpu(self.h_kappa)
        self.d_delta = parray.to_gpu(self.h_delta)
        self.d_bias = parray.to_gpu(self.h_bias)
        
        self.d_v0 = parray.to_gpu(self.h_v0)
        self.d_time_count = parray.to_gpu(self.h_time_count)
    
    
    def encode(self, neural_inputs, startbias = 0, avg_rate = 0.1):
        """
            Encode with IAFs
            
            Parameters
            ----------
            neural_inputs : PitchArray 
                PitchArray of shape (num_samples, num_neurons) containing inputs to all neurons
            startbias : integer, optional 
                the neuron index corresponding to first column of neural_inputs
            avg_rate: float, optional 
                average spiking rate assumed for neurons, will allocate memory num_samples/avg_rate for each neuron for storing spikes
                
                If not specified, will be set to 0.1
            
            Returns
            -------
            spikes: ndarray of self.dtype 
                 stores the spikes for one neuron after another
            spike_count: ndarray of int32 of size num_neurons 
                 indicates the number of spikes generated by each neuron
            
            Notes
            -----
            spikes for neuron j can be accessed by
            ::

                cum_count = np.concatenate((np.zeros(1,np.int32),np.cumsum(spike_count)))
                tk = spikes[cum_count[j]:cum_count[j+1]]
            
             """
        
        neuron_per_block=64
        
        if self.num_neurons != neural_inputs.shape[1]:
            raise ValueError("input size should match number of neurons")
        
        Ntimesteps = neural_inputs.shape[0]
        
        d_spikecount = parray.empty((1, self.num_neurons), np.int32)
        d_spike = parray.empty( ( int(np.ceil(Ntimesteps / avg_rate)), self.num_neurons), self.dtype) 
        
        if neural_inputs.__class__ is np.ndarray:
            d_neural_inputs = parray.to_gpu(neural_inputs)
        else:
            d_neural_inputs = neural_inputs
        
        
        launch_kernel(self.func, (neuron_per_block, 1, 1), (int(np.ceil(np.float64(self.num_neurons) / neuron_per_block)), 1), \
                      [neural_inputs, neural_inputs.ld, self.num_neurons, Ntimesteps, d_spike, d_spike.ld, [self.d_v0, startbias], \
                       [self.d_kappa, startbias], [self.d_bias, startbias], [self.d_delta, startbias], [self.d_time_count, startbias], \
                       d_spikecount, int(np.ceil(Ntimesteps / avg_rate)), self.dt], shared = self.dtype.itemsize * neuron_per_block)
        
        
        spike_count = d_spikecount.get()
        spike_count.resize((self.num_neurons,))
        
        if spike_count.max() >= np.ceil(Ntimesteps / avg_rate):
            raise ValueError("number of spikes exceeded the limit of buffer")
        
        spike = rearrange_spikes(d_spike, spike_count, self.num_neurons)
        
        return spike, spike_count
    
    def reset_timer(self):
        """Reset the time_count to zeros """
        self.d_time_count.fill(0)


class IAF_encode_rt(pop_encode):
    def __init__(self, num_neurons, dt, dtype = np.float64):
        """
            Population encoding with IAF neurons with random thresholds
            
            Parameters
            ----------
            num_neurons : integer
                number of neurons to encode
            dt : float 
                time interval between two consecutive samples in the input for each neuron
            dtype : type, optional 
                dtype of spike interval to be stored ( np.float32 not tested)
            
            """
        pop_encode.__init__(self,  num_neurons, dt, dtype)
        self.func = get_IAF_kernel_linear_rt(self.dtype)
    
    
    
    def set_time_count(self):
        """ Set time_count to zeros """
        self.h_time_count = np.zeros((self.num_neurons,1), self.dtype)
    
    def set_parameters(self):
        """
            Set the parameters using default values
            
            Will set kappa's to be 1.0, deltas to be 0.03, bias to be 0.8
            
            """
        
        self.h_kappa = np.zeros(self.num_neurons, self.dtype) + 1.0
        
        self.h_delta = np.zeros(self.num_neurons, self.dtype) + 0.03
        
        self.h_bias = np.zeros(self.num_neurons, self.dtype) + 0.8
        self.h_sigma = np.zeros(self.num_neurons, self.dtype) + 0.0003
        
        self.h_delta_value = np.random.normal(size=self.num_neurons) * self.h_sigma + self.h_delta
    
    def set_initial_value(self):
        """ set integration from zeros """
        #self.h_v0 = np.zeros(self.num_neurons, self.dtype)
        self.h_v0 = np.random.uniform(0,1,self.num_neurons).astype(self.dtype) * self.h_delta_value
    
    def load_parameters(self, h_kappa = None, h_bias = None, h_delta = None, h_sigma = None, h_delta_value = None, h_time_count = None, h_v0 = None):
        """
            Load encoding parameters to GPU
            
            h_kappa, h_bias, h_delta can be set up using default values (using None),
            or specified together (using ndarrays of dtype)
            
            h_time_count can be set up using default values (using None), or specified values(using ndarrays of dtype)
            
            h_v0 can be set up using default values (using None), or specified values(using ndarrays of dtype)
            """
        
        if h_kappa is None:
            self.set_parameters()
        else:
            self.h_kappa = h_kappa
            self.h_delta = h_delta
            self.h_bias = h_bias
            self.h_sigma = h_sigma
            self.h_delta_value = h_delta_value
        
        if h_time_count is None:
            self.set_time_count()
        else:
            self.h_time_count = h_time_count
        
        
        if h_v0 is None:
            self.set_initial_value()
        else:
            self.h_v0 = h_v0
        
        
        self.d_delta_value = parray.to_gpu(self.h_delta_value)
        
        self.d_kappa = parray.to_gpu(self.h_kappa)
        self.d_delta = parray.to_gpu(self.h_delta)
        self.d_bias = parray.to_gpu(self.h_bias)
        self.d_sigma = parray.to_gpu(self.h_sigma)
        
        self.d_v0 = parray.to_gpu(self.h_v0)
        self.d_time_count = parray.to_gpu(self.h_time_count)
    
    
    def encode(self, neural_inputs, startbias = 0, avg_rate = 0.1):
        """
            Encode with IAFs with random thresholds
            
            Parameters
            ----------
            neural_inputs : PitchArray 
                 PitchArray of shape (num_samples, num_neurons) containing inputs to all neurons
            startbias : integer 
                 the neuron index corresponding to first column of neural_inputs
            avg_rate : float 
                 average spiking rate assumed for neurons, will allocate memory num_samples/avg_rate for each neuron for storing spikes
            
            Returns
            -------
            spikes : ndarray of self.dtype 
                 stores the spikes for one neuron after another
            spike_count : ndarray of int32 of size num_neurons 
                 indicates the number of spikes generated by each neuron
            
            Notes
            -----
            spikes for neuron j can be accessed by
            ::
            
                 cum_count = np.concatenate((np.zeros(1,np.int32),np.cumsum(spike_count)))
                 tk = spikes[cum_count[j]:cum_count[j+1]]
            
            """
        
        neuron_per_block=64
        
        if self.num_neurons != neural_inputs.shape[1]:
            raise ValueError("input size should match number of neurons")
        
        Ntimesteps = neural_inputs.shape[0]
        
        d_spikecount = parray.empty((1, self.num_neurons), np.int32)
        
        randnum = np.random.normal(size = ( int(np.ceil(Ntimesteps / avg_rate)), self.num_neurons)).astype(self.dtype)
        #d_spike = parray.empty( ( int(np.ceil(Ntimesteps / avg_rate)), self.num_neurons), self.dtype) 
        d_spike = parray.to_gpu(randnum)
        
        
        if neural_inputs.__class__ is np.ndarray:
            d_neural_inputs = parray.to_gpu(neural_inputs)
        else:
            d_neural_inputs = neural_inputs
        
        
        launch_kernel(self.func, (neuron_per_block, 1, 1), (int(np.ceil(np.float64(self.num_neurons) / neuron_per_block)), 1), \
                      [neural_inputs, neural_inputs.ld, self.num_neurons, Ntimesteps, d_spike, d_spike.ld, [self.d_v0, startbias], \
                       [self.d_kappa, startbias], [self.d_bias, startbias], [self.d_delta, startbias], [self.d_time_count, startbias],\
                       d_spikecount, int(np.ceil(Ntimesteps / avg_rate)), self.dt, [self.d_delta_value,startbias], [self.d_sigma,startbias]], shared = self.dtype.itemsize * neuron_per_block)
        
        
        spike_count = d_spikecount.get()
        spike_count.resize((self.num_neurons,))
        
        if spike_count.max() >= np.ceil(Ntimesteps / avg_rate):
            raise ValueError("number of spikes exceeded the limit of buffer")
        
        spike = rearrange_spikes(d_spike, spike_count, self.num_neurons)
        
        return spike, spike_count
    
    def reset_timer(self):
        """reseting the time_count to zeros """
        self.d_time_count.fill(0)

def rearrange_spikes(d_spike, spike_count, num_neurons):
    spike = d_spike.get()
    sk = []
    for i in range(num_neurons):
        sk.append(spike[0:spike_count[i],i].copy())
    return sk



def get_IAF_kernel_positive(dtype = np.float64):
    IAF_linear_positive = """
        __global__ void
        ensemble_encode(%(type)s* g_input,
        int input_ld,
        int num_neurons,
        int size,
        %(type)s* g_spike,
        int spike_ld,
        %(type)s* g_v0,
        %(type)s* g_kappa,
        %(type)s* g_bias,
        %(type)s* g_delta,
        %(type)s* g_time_count,
        int* g_spike_count,
        int max_spike,
        %(type)s dt) 
        {
        int tid = threadIdx.x;
        int bdim = blockDim.x;
        int bid = blockIdx.x;
        int btIdx_to_neuIdx = bdim * bid + tid;
        
        
        %(type)s kappa, bias, ddt, v0, time_count;
        %(type)s delta;
        int spike_count;
        
        
        if(btIdx_to_neuIdx < num_neurons)
        {
        
        
		kappa=g_kappa[btIdx_to_neuIdx];
        
		bias=g_bias[btIdx_to_neuIdx];
        
		ddt=dt;
		v0=g_v0[btIdx_to_neuIdx];
		spike_count=0;
		time_count=g_time_count[btIdx_to_neuIdx];
        
		delta=g_delta[btIdx_to_neuIdx];
        
        }
        
        
		for(unsigned int i = 0; i < size - 1; ++i)
		{
        if(btIdx_to_neuIdx < num_neurons)
        {
        %(type)s y1 = (g_input[i * input_ld + btIdx_to_neuIdx] + bias) / kappa;
        %(type)s y2 = (g_input[(i+1) * input_ld + btIdx_to_neuIdx] + bias) / kappa;
        
        %(type)s area = (y1 + y2) / 2 * ddt;
        
        while(v0 + area >= delta)
        {
        %(type)s a = ( (y2-y1)/ ddt);
        
        %(type)s remain = (fabs(a) <= 1e-12)? ((delta - v0) / y1) : (sqrt(y1 * y1 + 2 * (delta - v0) * a) - y1)/ (a);
        
        g_spike[min(max_spike-1,spike_count) * spike_ld + btIdx_to_neuIdx] = time_count + remain;
        time_count += remain;
        spike_count++;
        
        
        area -= (delta - v0);
        
        v0 = 0;
        ddt -= remain; 
        y1 = y1 + remain * a; // y1 += remain * (y2-y1)/ddt;
        }
        
        v0 += area;
        time_count += ddt;
        ddt = dt;
        }
		}
        
		__syncthreads();
		
		g_v0[btIdx_to_neuIdx]=v0;
		g_time_count[btIdx_to_neuIdx]=time_count;
		g_spike_count[btIdx_to_neuIdx]=spike_count;
        
        
		
        }
        """
    func = func_compile("ensemble_encode", IAF_linear_positive % {"type": dtype_to_ctype(dtype)})
    return func


def get_IAF_kernel_linear(dtype = np.float64):
    IAF_linear = """
        
        __global__ void
        ensemble_encode(%(type)s* g_input,
        int input_ld,
        int num_neurons,
        int size,
        %(type)s* g_spike,
        int spike_ld,
        %(type)s* g_v0,
        %(type)s* g_kappa,
        %(type)s* g_bias,
        %(type)s* g_delta,
        %(type)s* g_time_count,
        int* g_spike_count,
        int max_spike,
        %(type)s dt)  
        {
        int tid = threadIdx.x;
        int bdim = blockDim.x;
        int bid = blockIdx.x;
        int btIdx_to_neuIdx = bdim * bid + tid;
        
		
        %(type)s kappa, bias, ddt, v0, time_count;
        %(type)s delta;
        int spike_count;
        
        
        if(btIdx_to_neuIdx<num_neurons)
        {
        
        
		kappa=g_kappa[btIdx_to_neuIdx];
        
		bias=g_bias[btIdx_to_neuIdx];
        
		ddt=dt;
		v0=g_v0[btIdx_to_neuIdx];
		spike_count=0;
		time_count=g_time_count[btIdx_to_neuIdx];
        
		delta=g_delta[btIdx_to_neuIdx];
        
        }
        
        
		for(unsigned int i = 0; i < size - 1; ++i)
		{
        if(btIdx_to_neuIdx < num_neurons)
        {
        %(type)s y1 = (g_input[i * input_ld + btIdx_to_neuIdx] + bias) / kappa;
        %(type)s y2 = (g_input[(i+1) * input_ld + btIdx_to_neuIdx] + bias) / kappa;
        
        if(y2 >= 0 || (y2<0 && y1<=0) )
        {
        %(type)s area = (y1 + y2) / 2 * ddt;
        
        while(v0 + area >= delta)
        {
        %(type)s a = ( (y2-y1)/ ddt);
        
        %(type)s remain = (fabs(a) <= 1e-12)? ((delta - v0) / y1) : (sqrt(y1 * y1 + 2 * (delta - v0) * a) - y1)/ (a);
        time_count += remain;
        g_spike[min(max_spike-1,spike_count) * spike_ld + btIdx_to_neuIdx] = time_count;
        
        spike_count++;
        area -= (delta - v0);
        v0 = 0;
        ddt -= remain; 
        y1 = y1 + remain * a; // y1 += remain * (y2-y1)/ddt;
        }
        
        v0 += area;
        time_count += ddt;
        ddt = dt;
        }else
        {
        %(type)s a = ( (y2-y1)/ ddt);
        %(type)s remain;
        while((remain = y1*y1 + 2*(delta - v0)*a) >= 0)
        {
        remain = (sqrt(remain) - y1) / a;
        
        time_count += remain;
        g_spike[min(max_spike-1,spike_count) * spike_ld + btIdx_to_neuIdx] = time_count;
        
        spike_count++;
        v0 = 0;
        ddt -= remain; 
        y1 = y1 + remain * a;
        a = ( (y2-y1) /ddt);
        }
        v0 += (y1+ y2) / 2 * ddt;
        time_count += ddt;
        ddt = dt;
        }
        
        
        }
		}
        
		__syncthreads();
		
		g_v0[btIdx_to_neuIdx]=v0;
		g_time_count[btIdx_to_neuIdx]=time_count;
		g_spike_count[btIdx_to_neuIdx]=spike_count;
        
        
		
        }
        """
    func = func_compile("ensemble_encode", IAF_linear % {"type": dtype_to_ctype(dtype)})
    return func


def get_IAF_kernel_linear_rt(dtype = np.float64):
    IAF_linear = """
        
        __global__ void
        ensemble_encode(%(type)s* g_input,
        int input_ld,
        int num_neurons,
        int size,
        %(type)s* g_spike,
        int spike_ld,
        %(type)s* g_v0,
        %(type)s* g_kappa,
        %(type)s* g_bias,
        %(type)s* g_delta,
        %(type)s* g_time_count,
        int* g_spike_count,
        int max_spike,
        %(type)s dt,
        %(type)s* g_delta_value,
        %(type)s* g_sigma)  
        {
        int tid = threadIdx.x;
        int bdim = blockDim.x;
        int bid = blockIdx.x;
        int btIdx_to_neuIdx = bdim * bid + tid;
        
        
        %(type)s kappa, bias, ddt, v0, time_count;
        %(type)s delta,sigma, delta_mean;
        int spike_count;
        
        
        if(btIdx_to_neuIdx<num_neurons)
        {
        
        
        kappa=g_kappa[btIdx_to_neuIdx];
        
        bias=g_bias[btIdx_to_neuIdx];
        
        ddt=dt;
        v0=g_v0[btIdx_to_neuIdx];
        spike_count=0;
        time_count=g_time_count[btIdx_to_neuIdx];
        
        delta_mean=g_delta[btIdx_to_neuIdx];
        delta = g_delta_value[btIdx_to_neuIdx];
        sigma = g_sigma[btIdx_to_neuIdx];
        }
        
        
        for(unsigned int i = 0; i < size - 1; ++i)
        {
        if(btIdx_to_neuIdx < num_neurons)
        {
        %(type)s y1 = (g_input[i * input_ld + btIdx_to_neuIdx] + bias) / kappa;
        %(type)s y2 = (g_input[(i+1) * input_ld + btIdx_to_neuIdx] + bias) / kappa;
        
        if(y2 >= 0 || (y2<0 && y1<=0) )
        {
        %(type)s area = (y1 + y2) / 2 * ddt;
        
        while(v0 + area >= delta)
        {
        %(type)s a = ( (y2-y1)/ ddt);
        
        %(type)s remain = (fabs(a) <= 1e-12)? ((delta - v0) / y1) : (sqrt(y1 * y1 + 2 * (delta - v0) * a) - y1)/ (a);
        time_count += remain;
        delta = g_spike[min(max_spike-1,spike_count) * spike_ld + btIdx_to_neuIdx] * sigma + delta_mean;
        g_spike[min(max_spike-1,spike_count) * spike_ld + btIdx_to_neuIdx] = time_count;
        
        spike_count++;
        area -= (delta - v0);
        v0 = 0;
        ddt -= remain; 
        y1 = y1 + remain * a; // y1 += remain * (y2-y1)/ddt;
        }
        
        v0 += area;
        time_count += ddt;
        ddt = dt;
        }else
        {
        %(type)s a = ( (y2-y1)/ ddt);
        %(type)s remain;
        while((remain = y1*y1 + 2*(delta - v0)*a) >= 0)
        {
        remain = (sqrt(remain) - y1) / a;
        
        time_count += remain;
        delta = g_spike[min(max_spike-1,spike_count) * spike_ld + btIdx_to_neuIdx] * sigma + delta_mean;
        g_spike[min(max_spike-1,spike_count) * spike_ld + btIdx_to_neuIdx] = time_count;
        
        spike_count++;
        v0 = 0;
        ddt -= remain; 
        y1 = y1 + remain * a;
        a = ( (y2-y1) /ddt);
        }
        v0 += (y1+ y2) / 2 * ddt;
        time_count += ddt;
        ddt = dt;
        }
        
        
        }
        }
        
        __syncthreads();
        
        g_v0[btIdx_to_neuIdx]=v0;
        g_time_count[btIdx_to_neuIdx]=time_count;
        g_spike_count[btIdx_to_neuIdx]=spike_count;
        g_delta_value[btIdx_to_neuIdx] = delta;
        
        
        }
        """
    func = func_compile("ensemble_encode", IAF_linear % {"type": dtype_to_ctype(dtype)})
    return func



